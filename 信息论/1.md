# Lecture 1

**Assumption**:

1. Infinite communication
2. A prob distribution over the messages

**Goal**:
Minimize average code length

**Prefix-Free Codes And Uniquely Decodable Codes**:

1. Prefix-free is a sufficient condition
2. Prefix-free is not a necessary condition

A: prefix-free codes
B: uniquely decodable codes
$A\subsetneqq B$

$$
\min_{C\in B}E[l(C)]=\min_{C\in A}E[l(C)]
$$

Problem:
$$
\begin{align}
&\forall C^*\in B,s.t.E[l(C^*)]\text{ achieves the minimum},\\
&\exists C'\in A, E[l(C')]=E[l(C^*)].
\end{align}
$$

**Kraft Inequality for Prefix-free Codes**
Assume $C=(c_1,\cdots,c_n)$ is prefix-free. Let $l_1,\cdots,l_n$ be the length(number of bits) of $c_1,\cdots,c_n$, then
$$
\sum_{i=1}^n2^{-l_i}\leq1
$$

**Minimal Average Code Length**
Setting: Message $M=\{m_1,\cdots,m_n\}$. Prob distribution $P=\{p_1,\cdots,p_n\}$.
Goal: Find prefix-free $C=\{c_1,\cdots,c_n\}$, with length $l_1,\cdots,l_n$
$$
\begin{align}
&\min_{l_1,\cdots,l_n}\sum_{i=1}^np_il_i\\
&s.t.\sum_{i=1}^n2^{-l_i}\leq1,l_i\geq0\\
\end{align}
$$
Answer: Let $l_i=-\log_2p_i,\sum p_il_i\geq\sum p_i\log_2\frac{1}{p_i}$. And if we let $l_i\in\mathbb{N}$, the difference is no more than 1.

**Entropy**
Given a random source $X$(random variable), with prob $(p_1,\cdots,p_n)$.
The entropy of $X$ is
$$
H(X)=\sum_{i=1}^n p_i\log_2\frac{1}{p_i}
$$

1. Minimum code length(description length)
2. Quantify information
3. Uniform distribution, $H(X)$ maximum; deterministic, $H(X)=0$
4. $H$ measures uncertainty of $X$
